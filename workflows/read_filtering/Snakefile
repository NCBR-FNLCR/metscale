'''
Author: Phillip Brooks, Charles Reid
Affiliation: UC Davis Lab for Data Intensive Biology
Objective: A Snakemake workflow to process reads to produce quality trimmed data 
Date: 2018-06-08
Documentation: docs/workflow_readfilt.md
'''

#from utils import container_image_is_external, container_image_name
from common.utils  import container_image_is_external, container_image_name 
from os.path import join, isfile, dirname
import os, re, gzip, io, glob, shutil
import pdb
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

HTTP = HTTPRemoteProvider()



###################################
# Read Filtering: default config
data_dir = config['data_dir']
sing_dir = config['sing_dir']
biocontainers = config['biocontainers']
taxclass = config['taxonomic_classification']
assembly = config['assembly']
readfilt = config['read_filtering']


###################################
# Read Filtering: build rules

# Skip to the very end of the file 
# to see the high-level build rules
# that trigger cascades of workflow
# tasks.



###################################
# Code to get samples input files based on sample name and glob file pattern

#Using user defined params get input file list
workflows = config['workflows']
all_sample_names = workflows['samples_input_workflow']['samples']
sample_input_files = []
for sample in all_sample_names:
    input_file_pattern = readfilt['read_patterns']['pre_trimming_glob_pattern']
    input_file_pattern = re.sub(r"\*", sample, input_file_pattern, 1)
    input_file_pattern = join(data_dir, input_file_pattern)
    sample_input_files.extend(glob.glob(input_file_pattern))

#nnow we need to remove file extension and data_dir
file_names = []
for file in sample_input_files:
    file_path_no_dir = file.replace(data_dir+'/','')
    file_names.append(file_path_no_dir.replace(readfilt["quality_trimming"]["sample_file_ext"],''))
sample_input_files = file_names
if (sample_input_files == []):
    print("Warning: No input files found!")





### ############################################
### # Read Filtering: bypass trimming, download trimmed data directly
### #
### # THIS RULE SHOULD PROBABLY BE COMMENTED OUT
### #
### # Note: either quality_trimming or download_trimmed_data
### # must be enabled, but not both. Otherwise you get conflicts
### # due to two rules producing the same output file.
### # 
### # download_trimmed_data is for testing, folks will not normally
### # have already-trimmed data to download.
### 
### post_trimmed_pattern = readfilt['read_patterns']['post_trimming_pattern']
### post_trimmed_relative_path = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
### 
### def download_reads_trimmed_data_url(wildcards):
###     """
###     Given a set of wildcards, return the URL where the 
###     post-trimmed reads can be downloaded (if available).
###     """
###     # Get the filename only from the relative path, and do wildcard substitution
###     post_trimmed_name = post_trimmed_pattern.format(**wildcards)
### 
###     # Get the URL where this file is available
###     read_data_url = config['files'][post_trimmed_name]
### 
###     return read_data_url
### 
### def download_reads_trimmed_data_file(wildcards):
###     """
###     Return the post-trimming file that matches the given wildcards
###     """
###     # Get the relative path and do wildcard substitution
###     post_trimming_file = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
###     return post_trimming_file.format(**wildcards)
### 
### 
### rule download_trimmed_data:
###     """
###     Fetch user-requested files from OSF containing trimmed reads that will be
###     used in various workflows.
### 
###     Note that this defines wildcard-based download rules, rather than
###     downloading all files all at once, to keep things flexible and fast.
###     """
###     output:
###         post_trimmed_relative_path
###     message:
###         """--- Skipping read trimming step, downloading trimmed reads directly."""
###     params:
###         trimmed_data_url = download_reads_trimmed_data_url,
###         trimmed_data_file = download_reads_trimmed_data_file
###     shell:
###         'wget -O {params.read_data_file} {params.read_data_url}'



###################################
# Read Filtering: build rules


