---
title: "Metagenomics Summary Report"
# author: "Stephen Turner"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_download: true
    code_folding: hide
    # out.width: "100%"
    fig.width: 9
    fig.height: 3
    warning: FALSE
    message: FALSE
    # theme: default
    # theme: cerulean
    # theme: journal
    # theme: flatly
    # theme: readable
    # theme: spacelab
    # theme: united
    # theme: cosmo
    # theme: lumen
    # theme: paper
    # theme: sandstone
    # theme: simplex
    # theme: yeti
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: false
      smooth_scroll: false
params:
  rmd: "report.Rmd"
---

```{r setup, message=FALSE, warning=FALSE}
# setup and load necessary libraries
library(knitr)
library(tidyverse)
library(DT)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
theme_set(theme_bw())


# Get input dir
input_dir <- snakemake@params[1]
# Set sample name (parameterized)
id <- snakemake@params[2]
```

<big>
<big>
Sample ID: <mark>**`r id`**</mark>
</big>
</big>


# About {.tabset .tabset-fade}

## Workflow overview

**More info: <https://github.com/signaturescience/metagenomics>**

These open source metagenomics workflows are intended to analyze the biological contents of complex environmental samples. The expected input is paired-end Illumina FASTQ files, and the current outputs include filtered reads, assembled contigs, MultiQC reports for FastQC and QUAST results, metagenome comparison estimates, taxonomic classifications, and gene predictions.


![Analysis flowchart](https://raw.githubusercontent.com/signaturescience/metagenomics/master/documentation/figures/Overview_Flowchart.png)

## About this analysis

<!-- <mark>Pull some information from here about the samples run, etc. from the logs, command history, etc.</mark> -->

Analysis completed: **`r file.info(".snakemake_timestamp")$ctime`**.

Typical command used to run this analysis:

```sh
snakemake --cores 24 --use-singularity \
    read_filtering_multiqc_workflow \
    read_filtering_khmer_count_unique_kmers_workflow \
    assembly_multiqc_workflow \
    comparison_output_heatmap_plots_all_workflow \
    tax_class_gather_workflow \
    tax_class_visualize_krona_kaijureport_workflow \
    functional_with_srst2_workflow \
    functional_prokka_with_megahit_workflow \
    functional_prokka_with_metaspades_workflow \
    functional_abricate_with_megahit_workflow \
    functional_abricate_with_metaspades_workflow

snakemake --use-singularity  post_processing_move_samples_dir_workflow
```

## About this report

This report pulls in statistics and reports generated after running all workflows as described in the version 1.2 wiki. Each major section (QA, taxonomic classification, assembly, functional annotation) is accessible via a link in the table of contents above. Tabs under each major heading expand to show summary results, links to further reports, details, or documentation on how these results were produced. Within particular tabs there are buttons that further subdivide results that are displayed (e.g., different assemblers, or different AMR tools). 

# Read Filtering {.tabset .tabset-fade}

## Procedure

The tools within this workflow perform read filtering and adapter trimming with Trimmomatic version 0.36, quality assessment of paired-end Illumina reads with FastQC version 0.11.7 and MultiQC version 1.4, and additional read processing options with scripts from Khmer version 2.1. 

**More info: <https://github.com/signaturescience/metagenomics/wiki/05.-Read-Filtering>**

![Read trimming workflow](https://raw.githubusercontent.com/signaturescience/metagenomics/master/documentation/figures/Read_Filtering_Flowchart.png)



## General statistics

```{r}
read_tsv(paste0(id, "_fastqc_multiqc_report_data/multiqc_general_stats.txt")) %>% 
  set_names(~str_remove(., "FastQC_mqc-generalstats-")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="General QA statistics about samples in this run.")
```


```{r}
get_unique_kmers <- function(file) {
  read_lines(file) %>% 
    grep("unique k-mers", ., value=TRUE) %>% 
    str_extract("\\d+") %>% 
    as.integer()
}
list.files(pattern="uniqueK\\d\\d\\.txt") %>% 
  tibble(file=.) %>% 
  mutate(sample=str_extract(file, ".+trim\\d+")) %>% 
  mutate(K=str_extract(file, "K\\d+")) %>% 
  mutate(number_unique_kmers=map_int(file, get_unique_kmers)) %>% 
  select(-file) %>% 
  mutate_if(is.integer, scales::comma) %>% 
  DT::datatable(caption="Number of unique kmers in each sample in this run.")
```


## MultiQC Reports: FastQC

Click for links to comprehensive quality assessment reports:

```{r, results='asis'}
x <- list.files(pattern="fastqc_multiqc_report\\.html$")
glue::glue("- **[{x}]({x})**") %>% 
  cat(sep="\n")
```



## Detailed FastQC statistics

```{r}
read_tsv(paste0(id, "_fastqc_multiqc_report_data/multiqc_fastqc.txt")) %>% 
  DT::datatable(caption="Comprehensive information from FastQC (side-scroll to see more)", options = list(scrollX='600px'))
```

```{r}
read_tsv(paste0(id, "_fastqc_multiqc_report_data/multiqc_sources.txt")) %>% 
  DT::datatable(caption="MultiQC Data Sources")
```


# Taxonomic Classification {.tabset .tabset-fade}

## Procedure

This workflow will perform taxonomic classification with sourmash version 2.0.0a3 and kaiju version 1.6.1 on filtered Illumina paired-end reads, as well as visualizations of kaiju results with KronaTools version 2.7. It has been tested to run offline following the execution of the Read Filtering Workflow. 

**More info: <https://github.com/signaturescience/metagenomics/wiki/08.-Taxonomic-Classification>**

![Taxonomic classification workflow](https://raw.githubusercontent.com/signaturescience/metagenomics/master/documentation/figures/Tax_Class_Flowchart.png)


## Krona plots {.tabset .tabset-pills}

### Links to existing HTML files

Click for links to krona diagrams:

```{r, results='asis'}
x <- list.files(pattern=".*krona.*html$")
glue::glue("- **[{x}]({x})**") %>% 
  cat(sep="\n")
```

### Direct Krona embedding 

```{r, results='asis'}
list.files(pattern="krona\\.html$") %>% 
  paste0("\n", ., ":\n\n", '<iframe src="', ., '" width="1000" height="1000"></iframe>\n\n') %>% 
  cat()
```


## Kaiju results {.tabset .tabset-pills}

### Direct links to files


```{r, results='asis'}
x <- list.files(pattern="kaiju.+summary$")
glue::glue("- **[{x}]({x})**") %>%
  cat(sep="\n")
```

### Inline reports

For brevity, the inline reports shown here only display the top 50 results for each sample. Please click the _"Direct links to files"_ button above to download and view the full results for each sample.

```{r}
# readLines(x[1]) %>% 
#   cat(sep="\n")
for (i in x) {
  cat("\n", i, "\n\n")
  cat(readLines(i, n=52), sep="\n")
  cat("\n\n\n\n")
}
```

## Sourmash Gather Detailed results {.tabset .tabset-pills}

### Direct links to spreadsheets

```{r, results='asis'}
x <- list.files(pattern="gather_output.csv$")
glue::glue("- **[{x}]({x})**") %>%
  cat(sep="\n")
```


### Trim2 k=21

```{r}
read_csv(paste0(id, "_trim2_k21.gather_output.csv")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="Sourmash Gather results for k=21 (side-scroll to see more)", options = list(scrollX='600px'))
```

### Trim2 k=31

```{r}
read_csv(paste0(id,"_trim2_k31.gather_output.csv")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="Sourmash Gather results for k=31 (side-scroll to see more)", options = list(scrollX='600px'))
```

### Trim2 k=51

```{r}
read_csv(paste0(id,"_trim2_k51.gather_output.csv")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="Sourmash Gather results for k=51 (side-scroll to see more)", options = list(scrollX='600px'))
```

### Trim30 k=21

```{r}
read_csv(paste0(id,"_trim30_k21.gather_output.csv")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="Sourmash Gather results for k=21 (side-scroll to see more)", options = list(scrollX='600px'))
```

### Trim30 k=31

```{r}
read_csv(paste0(id,"_trim30_k31.gather_output.csv")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="Sourmash Gather results for k=31 (side-scroll to see more)", options = list(scrollX='600px'))
```

### Trim30 k=51

```{r}
read_csv(paste0(id,"_trim30_k51.gather_output.csv")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="Sourmash Gather results for k=51 (side-scroll to see more)", options = list(scrollX='600px'))
```



# Assembly {.tabset .tabset-fade}

## Procedure

The tools within this workflow perform metagenome assemblies with the de novo assemblers metaSPAdes in SPAdes version 3.11.1, as well as MEGAHIT version 1.1.2, on trimmed Illumina paired-end reads. QUAST version 4.5 is used to evaluate the assemblies, and MultiQC version 1.4 provides aggregated visualizations for the QUAST reports of each assembler. This workflow has been tested to run offline in an air-gapped system following the execution of the Read Filtering Workflow.

**More info: <https://github.com/signaturescience/metagenomics/wiki/06.-Assembly>**

![Assembly workflow](https://raw.githubusercontent.com/signaturescience/metagenomics/master/documentation/figures/Assembly_Flowchart.png)


## General statistics {.tabset .tabset-pills}

### MEGAHIT

```{r}
read_tsv(paste0(id, ".megahit_multiqc_report_data/multiqc_general_stats.txt")) %>% 
  set_names(~str_remove(., "QUAST_mqc-generalstats-")) %>% 
  select(Sample, Total_length, N50) %>% 
  DT::datatable(caption="General assembly statistics about samples in this run from MEGAHIT.")
  
```

### MetaSPADES

```{r}
read_tsv(paste0(id, ".metaspades_multiqc_report_data/multiqc_general_stats.txt")) %>% 
  set_names(~str_remove(., "QUAST_mqc-generalstats-")) %>% 
  select(Sample, Total_length, N50) %>% 
  DT::datatable(caption="General assembly statistics about samples in this run from MetaSPADES.")
```

## QUAST {.tabset .tabset-pills}

### MultiQC reports

Click for links to comprehensive quality assessment reports:

```{r, results='asis'}
x <- list.files(pattern="(megahit|metaspades)_multiqc_report\\.html$")
glue::glue("- **[{x}]({x})**") %>% 
  cat(sep="\n")
```


### Individual QUAST reports

Individual QUAST reports:

```{r, results='asis'}
x <- list.files(pattern="^report\\.html$", recursive=TRUE)
glue::glue("- **[{x}]({x})**") %>% 
  cat(sep="\n")
```


## Detailed results  {.tabset .tabset-pills}

### MEGAHIT

```{r}
read_tsv(paste0(id, ".megahit_multiqc_report_data/multiqc_quast.txt")) %>% 
  DT::datatable(caption="Comprehensive information from QUAST (side-scroll to see more)", options = list(scrollX='600px'))
  
```

### MetaSPADES

```{r}
read_tsv(paste0(id, ".metaspades_multiqc_report_data/multiqc_quast.txt")) %>% 
  DT::datatable(caption="Comprehensive information from QUAST (side-scroll to see more)", options = list(scrollX='600px'))
```


# Comparison {.tabset .tabset-fade}

## Procedure

In this workflow, sourmash version 2.0.0a3 performs multi-sample metagenome comparisons. This is done by first generating MinHash signatures on the filtered reads (i.e., the Read Filtering Workflow output) or the assembled contigs (i.e., the Assembly Workflow output) with sourmash compute. The sourmash compare functionality then compares signatures from multiple datasets to each other and outputs their pairwise comparisons as Jaccard indexes in a final *.csv file. Currently, this workflow is designed to compare MinHash signatures of different quality trimmed values and assemblers for the same sample and output results in csv files and heatmap visualizations. Different samples can also be compared to each other, although confounding factors like sequencing depth can obscure the direct comparison of k-mer content in different samples.

**More info: <https://github.com/signaturescience/metagenomics/wiki/07.-Comparison>**

![Comparison workflow](https://raw.githubusercontent.com/signaturescience/metagenomics/master/documentation/figures/Comparison_Flowchart.png)


## CSVs

```{r, results='asis'}
x <- list.files(pattern="read_assembly_comparison", recursive=FALSE)
glue::glue("- **[{x}]({x})**") %>% 
  cat(sep="\n")
```

## PNGs

### Direct links to images

```{r, results='asis'}
x <- list.files(pattern="read_assembly_comparison.+\\.png$", recursive=TRUE)
bx <- basename(x)
glue::glue("- **[{bx}]({x})**") %>% 
  cat(sep="\n")
```


### Inline graphics (in the same order as above)

```{r}
knitr::include_graphics(x)
```



# Functional Inference {.tabset .tabset-fade}

## Procedure

This workflow will perform functional inference (i.e., gene and plasmid detection) on filtered Illumina paired-end reads with SRST2 version 0.2.0, or on assembled contigs with prokka version 1.13.3 and ABRicate version 0.5. It has been tested to run offline after the execution of the Read Filtering Workflow and/or Assembly Workflow. Default prokka databases are used within the functional inference workflow, but users have the option of selecting from various pre-compiled databases for SRST2 and ABRicate.

**More info: <https://github.com/signaturescience/metagenomics/wiki/09.-Functional-Inference>**

![Functional inference workflow](https://raw.githubusercontent.com/signaturescience/metagenomics/master/documentation/figures/Functional_Flowchart.png)


## Prokka

Click below to access prokka results:

```{r, results='asis'}
x <- list.dirs() %>% 
  grep(pattern = "prokka_annotation", x=., value=TRUE) %>% 
  list.files(path=., pattern="\\.tsv", full.names = TRUE)
bx <- basename(x)
glue::glue("- **[{bx}]({x})**") %>% 
  cat(sep="\n")
```

## Antibiotic resistance {.tabset .tabset-pills}

### Abricate (contigs)

```{r, results='asis'}
x <- list.files(pattern="card\\.csv$")
glue::glue("- **[{x}]({x})**") %>% 
  cat(sep="\n")
```

### SRST2 (reads)

```{r, results='asis'}
x <- list.files(pattern="srst2.*results\\.txt$")
glue::glue("- **[{x}]({x})**") %>% 
  cat(sep="\n")
```




```{r, include=FALSE}
knitr::knit_exit()
```


# xx {.tabset .tabset-fade}

## General statistics

## Detailed results

## Procedure

Information goes here about the procedure, versions, etc. Lorem ipsum dolor sit amet, semper suscipit sea at. Dico scriptorem nec at, ex qui virtute dolores oportere. Duis tantas ponderum ut has. Est saepe mandamus salutatus et, id sed semper detracto moderatius, ei sit aperiam voluptua. Per esse justo fierent eu, duo quando tempor ut. At elitr doming possim vim, ut dolorem appetere nec.





# xx {.tabset .tabset-fade}

## General statistics

## Detailed results

## Procedure

Information goes here about the procedure, versions, etc. Lorem ipsum dolor sit amet, semper suscipit sea at. Dico scriptorem nec at, ex qui virtute dolores oportere. Duis tantas ponderum ut has. Est saepe mandamus salutatus et, id sed semper detracto moderatius, ei sit aperiam voluptua. Per esse justo fierent eu, duo quando tempor ut. At elitr doming possim vim, ut dolorem appetere nec.



