'''
Author: Chris Grahlmann
Affiliation: Signature Science
Objective: A Snakemake workflow to annotate prokaryotic genomes and screen for genes of interest
Date: Nov 28, 2018
Documentation: docs/workflow_assembly.md
'''

from common.utils  import container_image_is_external, container_image_name
from os.path import join, isfile, dirname
import os, re



############################################
# Inference: default config

data_dir = config['data_dir']
sing_dir = config['sing_dir']
image_dir = 'images/read'
biocontainers = config['biocontainers']
taxclass = config['taxonomic_classification']
assembly = config['assembly']
readfilt = config['read_filtering']
inference = config['functional_inference']


###################################
# Functional Inference: prokka with megahit input


prokka_image = container_image_name(biocontainers, 'prokka')

prokka_with_megahit_output_dir = join(data_dir,inference['prokka_with_megahit']['outdir_pattern'].format(sample='{sample}',
                            qual='{qual}'))

prokka_with_megahit_output_dir_sing = join(sing_dir,inference['prokka_with_megahit']['outdir_pattern'].format(sample='{sample}',
                            qual='{qual}'))
prokka_with_megahit_input_dir = join(data_dir,inference['prokka_with_megahit']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
prokka_with_megahit_input_dir_sing = join(sing_dir,inference['prokka_with_megahit']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
prokka_with_megahit_prefix_files = inference['prokka_with_megahit']['prefix_pattern']
prokka_megahit_threads = inference['prokka_with_megahit']['threads']


rule prokka_with_megahit:
    input:
        prokka_with_megahit_input_dir
    output:
        directory(prokka_with_megahit_output_dir)
    params:
        input_files = prokka_with_megahit_input_dir_sing,
        output_dir = prokka_with_megahit_output_dir_sing,
        prefix_files = prokka_with_megahit_prefix_files
    threads:
        prokka_megahit_threads
    singularity:
        prokka_image
    shell:
    	'export LC_ALL=C '
    	'&& '
        'prokka '
        '{params.input_files} '
        '--metagenome --eval 1e-06 --notrna --rnammer '
        '--outdir {params.output_dir} '
        '--prefix {params.prefix_files} '
        '--cpus {threads}'




###################################
# Functional Inference: prokka with metaspades input

prokka_with_metaspades_output_dir = join(data_dir,inference['prokka_with_metaspades']['outdir_pattern'].format(sample='{sample}',
                            qual='{qual}'))
prokka_with_metaspades_output_dir_sing = join(sing_dir,inference['prokka_with_metaspades']['outdir_pattern'].format(sample='{sample}',
                            qual='{qual}'))
prokka_with_metaspades_input_dir = join(data_dir,inference['prokka_with_metaspades']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
prokka_with_metaspades_input_dir_sing = join(sing_dir,inference['prokka_with_metaspades']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
prokka_with_metaspades_prefix_files = inference['prokka_with_metaspades']['prefix_pattern']
prokka_metaspades_threads = inference['prokka_with_metaspades']['threads']


rule prokka_with_metaspades:
    input:
        prokka_with_metaspades_input_dir
    output:
        directory(prokka_with_metaspades_output_dir)
    params:
        input_files = prokka_with_metaspades_input_dir_sing,
        output_dir = prokka_with_metaspades_output_dir_sing,
        prefix_files = prokka_with_metaspades_prefix_files
    threads:
        prokka_metaspades_threads
    singularity:
        prokka_image
    shell:
    	'export LC_ALL=C '
    	'&& '
        'prokka '
        '{params.input_files} '
        '--metagenome --eval 1e-06 --notrna --rnammer '
        '--outdir {params.output_dir} '
        '--prefix {params.prefix_files} '
        '--cpus {threads}'


###################################
# Functional Inference: abricate with megahit input

abricate_image = container_image_name(biocontainers, 'abricate')
abricate_with_megahit_output = join(data_dir,inference['abricate_with_megahit']['output_pattern'].format(sample='{sample}',
                            qual='{qual}', db='{db}'))
abricate_with_megahit_output_sing = join(sing_dir,inference['abricate_with_megahit']['output_pattern'].format(sample='{sample}',
                            qual='{qual}', db='{db}'))
abricate_with_megahit_input = join(data_dir,inference['abricate_with_megahit']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
abricate_with_megahit_input_sing = join(sing_dir,inference['abricate_with_megahit']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
abricate_with_megahit_db = '{db}'


rule abricate_with_megahit:
    input:
        abricate_with_megahit_input
    output:
        abricate_with_megahit_output
    params:
        input_files = abricate_with_megahit_input_sing,
        output_files = abricate_with_megahit_output_sing,
        gene_db = abricate_with_megahit_db
    singularity:
        abricate_image
    shell:
        'abricate --csv '
        '--db {params.gene_db} '
        '{params.input_files}>'
        '{params.output_files} '




###################################
# Functional Inference: abricate with metaspades input


abricate_with_metaspades_output = join(data_dir,inference['abricate_with_metaspades']['output_pattern'].format(sample='{sample}',
                            qual='{qual}', db='{db}'))
abricate_with_metaspades_output_sing = join(sing_dir,inference['abricate_with_metaspades']['output_pattern'].format(sample='{sample}',
                            qual='{qual}', db='{db}'))
abricate_with_metaspades_input = join(data_dir,inference['abricate_with_metaspades']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
abricate_with_metaspades_input_sing = join(sing_dir,inference['abricate_with_metaspades']['input_pattern'].format(sample='{sample}',
                            qual='{qual}'))
abricate_with_metaspades_db = '{db}'


rule abricate_with_metaspades:
    input:
        abricate_with_metaspades_input
    output:
        abricate_with_metaspades_output
    params:
        input_files = abricate_with_metaspades_input_sing,
        output_files = abricate_with_metaspades_output_sing,
        gene_db = abricate_with_metaspades_db
    singularity:
        abricate_image
    shell:
        'abricate --csv '
        '--db {params.gene_db} '
        '{params.input_files}>'
        '{params.output_files} '





###################################
# Functional Inference: SRST2

srst2_fwd_post_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}', qual='{qual}'))
srst2_rev_post_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}', qual='{qual}'))
srst2_post_trimming_inputs = [ srst2_fwd_post_trimmed, 
                        srst2_rev_post_trimmed]

srst2_fwd_post_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}', qual='{qual}'))
srst2_rev_post_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}', qual='{qual}'))
srst2_post_trimming_inputs_list = [ srst2_fwd_post_trimmed_sing, 
                        srst2_rev_post_trimmed_sing]

def srst2_post_trimming_inputs_sing(wildcards):
    # input already includes data/ prefix
    pre_inputs_wc = [x.format(**wildcards) for x in srst2_post_trimming_inputs_list]
    # for containers, turn prefix data/ into /data/
    pre_inputs_wc = ["/%s"%(x) for x in pre_inputs_wc]
    pre_inputs_wcs = " ".join(pre_inputs_wc)
    return pre_inputs_wc

srst2_db_location = join(sing_dir, '{db}')
srst2_output_sing = join(sing_dir, inference['srst2']['output_pattern'].format(sample='{sample}', qual='{qual}', db='{db}'))
#SRST2 likes to append some strings that are difficult to replicate with the db wildcard. So decided to simplify and just look for the log file.
srst2_log_out = "data/{sample}_trim{qual}_{db}.srst2.log"

def srst2_output_sub_sing(wildcards):
    return srst2_output_sing.format(**wildcards)


srst2_image = container_image_name(biocontainers, 'srst2')
srst2_threads = inference['srst2']['threads']


rule srst2_with_raw_reads:
    input:
       srst2_post_trimming_inputs
    output:
        srst2_log_out
    params:
        input_files = srst2_post_trimming_inputs_sing,
        output_files = srst2_output_sub_sing,
        db = srst2_db_location
    threads:
        srst2_threads
    singularity:
        srst2_image
    shell:
        'srst2 '
        '--input_pe {params.input_files} '
        '--output {params.output_files} '
        '--log --gene_db {params.db} '
        '--threads {threads} '
        '--min_coverage 0 '

  

###################################
# Functional Inference: build rules

workflows = config['workflows']

directions = [readfilt['direction_labels']['forward'],
              readfilt['direction_labels']['reverse']]


rule functional_prokka_with_megahit_workflow:
    """
    Run prokka with MEGAHIT contigs
    """
    input:
        expand( prokka_with_megahit_output_dir,
                sample    = sample_input_files,
                qual   = workflows['functional_prokka_with_megahit_workflow']['qual']
        )


rule functional_prokka_with_metaspades_workflow:
    """
    Run prokka with metaspades contigs
    """
    input:
        expand( prokka_with_metaspades_output_dir,
                sample    = sample_input_files,
                qual   = workflows['functional_prokka_with_metaspades_workflow']['qual']
        )


rule functional_abricate_with_megahit_workflow:
    """
    Run abricate with assembled megahit contigs as input
    """
    input:
        expand( abricate_with_megahit_output,
                sample    = sample_input_files,
                qual   = workflows['functional_abricate_with_megahit_workflow']['qual'],
                db     = workflows['functional_abricate_with_megahit_workflow']['db'],
        )


rule functional_abricate_with_metaspades_workflow:
    """
    Run abricate with assembled metaspades contigs as input
    """
    input:
        expand( abricate_with_metaspades_output,
                sample    = sample_input_files,
                qual   = workflows['functional_abricate_with_metaspades_workflow']['qual'],
                db     = workflows['functional_abricate_with_metaspades_workflow']['db'],
        )


rule functional_with_srst2_workflow:
    '''
    Run srst2 on raw reads
    '''
    input:
       expand( srst2_log_out,
                sample    = sample_input_files,
                qual      = workflows['functional_with_srst2_workflow']['qual'],
                db        = workflows['functional_with_srst2_workflow']['db'],
                direction = directions,
        )

